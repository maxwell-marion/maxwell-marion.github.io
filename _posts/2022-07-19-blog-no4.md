ST 558 - Fourth Blog Post
================
Maxwell Marion-Spencer
2022-07-19

## Favorite Modeling Method: Boosted Tree

The modeling method I found the most interesting was the boosted tree
method.

This method relies on growing trees in a sequential manner, where each
tree is based on an updated version of the previous tree, and the
predictions are updated as the trees grow. A benefit of this method is
that it is fairly slow, so you are unlikely to over fit the model.

There are four tuning parameters for a Boosted Tree model:

    n.trees: Number of trees, increasing n reduces the error on training set
    interaction.depth: number of splits it has to perform on a tree
    shrinkage: learning rate, reduce the impact of each additional fitted tree
    n.minobsinnode: minimum number of observations in treesâ€™ terminal nodes

## A Boosted Tree Example:

First, we will bring our necessary libraries and the `iris` data set to
use as our example data.

``` r
# Libraries
library(tidyverse)
library(caret)
library(gbm)

# Data set
data("iris")
```

Next, we will split our iris data into a test set and a training set.
With 70% of our values going to our training set, and 30% going into our
test set.

``` r
# Establishing train and test sets
set.seed(11)
train <- sample(1:nrow(iris),size=nrow(iris)*0.7)
test <- setdiff(1:nrow(iris),train)
irisTrain <- iris[train, ]
irisTest <- iris[test, ]
```

Typically we would conduct some exploratory data analysis on the
training set, in order to get an idea of the distributions and existing
relationships between the variables.

We can now fit our boosted tree model, with Sepal.Width, Petal.Length,
and Petal.Width being used to predict Sepal.Length.

We are using five-fold cross validation.

``` r
fit_boost <- train(Sepal.Length ~ Sepal.Width + Petal.Length + Petal.Width,
                 data = irisTrain,
                 method = "gbm",
                 preProcess = c("center", "scale"),
                 trControl = trainControl(method = "cv", number = 5),
                 tuneGrid = expand.grid(n.trees = c(50, 100, 150),
                                        interaction.depth = 1:3, 
                                        shrinkage = 0.1, 
                                        n.minobsinnode = 10),
                 verbose = FALSE)

fit_boost
```

    ## Stochastic Gradient Boosting 
    ## 
    ## 105 samples
    ##   3 predictor
    ## 
    ## Pre-processing: centered (3), scaled (3) 
    ## Resampling: Cross-Validated (5 fold) 
    ## Summary of sample sizes: 85, 84, 84, 82, 85 
    ## Resampling results across tuning parameters:
    ## 
    ##   interaction.depth  n.trees  RMSE       Rsquared   MAE      
    ##   1                   50      0.4257853  0.7572509  0.3466525
    ##   1                  100      0.4094079  0.7691416  0.3349695
    ##   1                  150      0.4031822  0.7723325  0.3310160
    ##   2                   50      0.4033323  0.7772925  0.3286625
    ##   2                  100      0.3954207  0.7795636  0.3254049
    ##   2                  150      0.3975531  0.7771400  0.3275899
    ##   3                   50      0.4044406  0.7769947  0.3299089
    ##   3                  100      0.3945772  0.7813833  0.3282434
    ##   3                  150      0.3962608  0.7739133  0.3305880
    ## 
    ## Tuning parameter 'shrinkage' was held constant at a value of 0.1
    ## Tuning parameter 'n.minobsinnode' was
    ##  held constant at a value of 10
    ## RMSE was used to select the optimal model using the smallest value.
    ## The final values used for the model were n.trees = 100, interaction.depth = 3, shrinkage = 0.1 and n.minobsinnode
    ##  = 10.

We can now apply our final model to our test set.

``` r
prediction <- round(postResample(predict(fit_boost, newdata = irisTest), obs = irisTest$Sepal.Length),4)
prediction
```

    ##     RMSE Rsquared      MAE 
    ##   0.4020   0.7485   0.3372

This prediction seems pretty good! The RMSE is on the lower end, though
we appear to have lost a bit of RSquared value compared to the
performance on the training data.

## Render Code
